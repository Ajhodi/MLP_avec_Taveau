{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462fe09b-613c-48ff-a8ff-bafaa27df769",
   "metadata": {},
   "source": [
    "# Protein Structure Prediction Using MLP \n",
    "# 1. Introduction\n",
    "This project aims to predict protein structures using *DSSP* encoding and *Multi-Layer Perceptron* (MLP) models. We based our project on an article by Burkhard Rost and Chris Sander (Prediction of Protein Secondary Structure at Better than 70% Accuracy, Journal of Molecular Biology, 1993), in which the authors describe their model implementation to achieve 70% accuracy. Essentially, the model was a two-layered feed-forward neural network containing a single hidden layer, and it was trained using a database of 130 water-soluble protein chains. Our project differs by utilizing modern deep learning architectures such as Keras and a DSSP database. Moreother, we use an *nvidia gtx 1650* for claculation. \n",
    "\n",
    "* Keras : <br>\n",
    "Keras is a software library that provides a Python interface for artificial neural networks. It acts as an abstraction layer for building and training deep learning models, making it easier for developers and researchers to implement complex neural networks without needing to delve into the underlying mathematical details. Keras is designed to be user-friendly, modular, and extensible, allowing for rapid experimentation and prototyping (Jaya Gupta et al., 2022).\n",
    "\n",
    "* DSSP : <br>\n",
    "DSSP, which stands for \"Define Secondary Structure of Proteins,\" is a widely used algorithm and software tool for assigning secondary structure to protein structures based on their three-dimensional coordinates. It was developed by Wolfgang Kabsch and Chris Sander in the 1980s and has since become a standard method in structural biology (Shaowen Yao et al., 2017).\n",
    "\n",
    "Finnaly, Key steps include:\n",
    "   - Feature extraction from protein sequence files.\n",
    "   - Preprocessing using one-hot encoding and frequency encoding.\n",
    "   - Resampling using SMOTE for class balancing.\n",
    "   - Implementing MLP for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c2ea8-7608-4587-befc-5c13bc8848f1",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing and Feature Extraction:\n",
    "   \n",
    "The original data file, as shown in this example, contained the protein residue sequences (RES) along with DSSP, DSSPACC (an extension of DSSP that incorporates accessibility information), STRIDE (Structural Identification), and the alignment.\n",
    "\n",
    "                                        RES:M,F,K,V,Y,G,Y,D,S,N,I,H,K,C,V\n",
    "                                        DSSP:_,E,E,E,E,E,_,_,T,T,T,S,_,_\n",
    "                                        DSSPACC:e,b,e,b,b,b,b,e,b,e,b,e,e\n",
    "                                        STRIDE:C,E,E,E,E,E,C,T,T,T,T,T,T\n",
    "                                        RsNo:1,2,3,4,5,6,7,8,9,10,11,12,13,14\n",
    "                                        DEFINE:E,E,E,E,E,E,_,_,_,_,_,_,_\n",
    "                                        align1:M,F,K,V,Y,G,Y,D,S,N,I,H,K\n",
    "                                        align2:K,I,E,V,Y,G,I,P,D,E,V,G,R\n",
    "<div style=\"text-align: center;\">\n",
    "    Example of aazb-1 protein used in the dataset \n",
    "</div>\n",
    "<br>\n",
    "Since Keras only processes numerical values, we needed to encode our sequences using one-hot encoding and frequency encoding.\n",
    "\n",
    "* One-Hot encoding : <br>\n",
    "    One-hot encoding is a method of converting categorical variables into a binary matrix representation. Since our protein sequences contain 20 residues, each amino acid will be encoded using 20 digits. Gaps and unrecognized amino acids will be encoded with a repetition of 20 zeros. Consequently, a DataFrame that we will use will have a significant size; for a peptide of 13 amino acids, we will have 260 columns.\n",
    "\n",
    "* Frequency encoding : <br>\n",
    "    In frequency encoding, we use the proportion of each amino acid to encode the entire protein. The final dataset will be much smaller in size compared to one-hot encoding, with only 20 columns instead of 260.\n",
    "\n",
    "Furthermore, the corresponding secondary structure will be encoded as follows: *{'H': 0, 'E': 1, 'C': 2}*. Note that every other character different from H and E will be coded as 2.\n",
    "\n",
    "The first step is to parse the files to retrieve the needed information and encode the sequences. We created the script *Extract_features.py* to handle all of the preprocessing. Additionally, if required, this script can perform further sampling via the *SMOTE()* function from the imbalanced-learn package to balance class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2001f94d-e831-466e-ab8c-c16d38d8988c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precessing files ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 513/513 [00:01<00:00, 344.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ...\n",
      "OneHot Encoding ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 77963/77963 [00:02<00:00, 28062.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe creation...\n",
      "Precessing files ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 513/513 [00:01<00:00, 351.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ...\n",
      "Frequences calculation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 77963/77963 [00:00<00:00, 112163.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe creation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DSSP</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CDAFVGTWKLVSS</th>\n",
       "      <td>2</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAFVGTWKLVSSE</th>\n",
       "      <td>2</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFVGTWKLVSSEN</th>\n",
       "      <td>2</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FVGTWKLVSSENF</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VGTWKLVSSENFD</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               DSSP         0    1         2  ...        16        17   18        19\n",
       "RES                                           ...                                   \n",
       "CDAFVGTWKLVSS     2  0.076923  0.0  0.000000  ...  0.076923  0.076923  0.0  0.153846\n",
       "DAFVGTWKLVSSE     2  0.076923  0.0  0.000000  ...  0.076923  0.076923  0.0  0.153846\n",
       "AFVGTWKLVSSEN     2  0.076923  0.0  0.076923  ...  0.076923  0.076923  0.0  0.153846\n",
       "FVGTWKLVSSENF     2  0.000000  0.0  0.076923  ...  0.076923  0.076923  0.0  0.153846\n",
       "VGTWKLVSSENFD     2  0.000000  0.0  0.076923  ...  0.076923  0.076923  0.0  0.153846\n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction example\n",
    "from Exctract_features import create_dataset\n",
    "\n",
    "pwd = \"./513_distribute\"\n",
    "\n",
    "df1 = create_dataset(pwd, method='ohe', rsp=True)  # One-hot encoding with resampling\n",
    "df2 = create_dataset(pwd, method='freq', rsp=True)  # Frequency encoding with resampling\n",
    "\n",
    "# Displaying a sample of the dataset\n",
    "df1.head()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b36d0-bf4b-4f8d-b479-635b3de33539",
   "metadata": {},
   "source": [
    "# 3.  Multi-Layer Perceptron (MLP) Implementation\n",
    "\n",
    "To implement our model using Keras (in *keras_MLP.ipynb*), we start by splitting the dataset into training and testing sets using *train_test_split()*, reserving 20% of the data for testing while ensuring reproducibility with a fixed random state. Next, we convert the target labels into a one-hot encoded format using *to_categorical()*, which is suitable for classification (the resulting format is *1.,0.,0.* for *H*, *0.,1.,0.* for *E*, and *0.,0.,1.* for *C*).\n",
    "\n",
    "Our model architecture consists of four dense layers, with the first three employing *ReLU* activation functions and the final layer using softmax to output class probabilities. We compile the model with the *Adam* optimizer and *categorical crossentropy loss*, enabling it to effectively learn and classify inputs based on the provided features. Finally, we added an early stopping mechanism to prevent the model from overfitting.\n",
    "\n",
    "We tested two architectures: the first one for one-hot encoding and the second for frequency encoding. We maintained the same architecture as the one used for one-hot encoding for the combined DataFrame (one-hot + frequency). The first architecture is simpler compared to the second one and is designed to learn quickly to achieve higher accuracy. Furthermore, we tested various layer densities and learning rates to achieve the results that we will discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f22c73-89cb-4738-a353-b0e84392bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='min')\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(500, activation='relu'),  \n",
    "    keras.layers.Dense(256, activation='relu'), \n",
    "    keras.layers.Dense(128, activation='relu'), \n",
    "    keras.layers.Dense(3, activation='softmax')                   \n",
    "])\n",
    "# For the second and third model we use 5 hiden layers (1000, 500, 500, 256, 128), Relu as activation\n",
    "opt = keras.optimizers.Adam(learning_rate=0.008) # for frequency encoding we use learning_rate=0.001, the same for the last case\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=3, batch_size=32, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5adec0-b8ff-4cad-b553-636f837aebc8",
   "metadata": {},
   "source": [
    "# 4. Results and Analysis\n",
    "   \n",
    "We implemented our model in three different ways: with one-hot encoding, frequency encoding, and a combination of both. We evaluated our model to identify which approach performed the best.\n",
    "\n",
    "## One-hot encoding\n",
    "As mentioned, one-hot encoding generates the largest amount of data for analysis, and our results showed an accuracy of 65.68%. As excpeted, our model learned very quickly; we achieved 60.18% accuracy after the first epoch and stopped after 3 epochs.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Imges/plot_ohe.png\">\n",
    "</div>\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Imges/matrix_ohe.png\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "## Frequency endocing\n",
    "Even though the DataFrame is smaller, the learning time increases, resulting in a better accuracy of 80.31% after 30 epochs.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Imges/plot_freq.png\">\n",
    "</div>\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Imges/matrix_freq.png\">\n",
    "</div>\n",
    "\n",
    "## Combine \n",
    "Combining the two datasets for training resulted in a larger DataFrame but a significant drop in accuracy compared to the previous methods. The model performed similarly to the one using one-hot encoding in the sense that it started at around 60% accuracy, but its learning curve varied dramatically throughout the training process. Ultimately, the model clearly overfitted, achieving an accuracy of only 64.10%.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Imges/plot_bith.png\">\n",
    "</div>\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Imges/matrix_bith.png\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26764fa-1c59-4411-895a-202d7b66a3aa",
   "metadata": {},
   "source": [
    "# 5. Discusssion and Conclusions\n",
    "\n",
    "Our three models did not perform the same. One-hot encoding allowed the model to learn very quickly but resulted in low accuracy as a drawback. Frequency encoding yielded the best results, albeit with a relatively long learning time. Finally, combining the two methods led to overfitting in the model. We also observed that precision and recall for each class were quite balanced across all models.\n",
    "\n",
    "We tested our models using balanced data, but when faced with imbalanced data, accuracy dropped significantly in all cases, with respective accuracies of 60.44%, 60.64%, and 60.49% (with overfitting) for one-hot encoding, frequency encoding, and the combined approach. Ultimately, when unbalanced, the model tended to become overly specific to the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b72e27-f7fe-4cdf-8f4b-37a5ecb7087d",
   "metadata": {},
   "source": [
    "# Suggestions for Further Improvement:\n",
    "\n",
    "We could have tested an autoencoding model using ProtBERT and the raw amino acid sequences, but we did not do so due to time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe524b4-769b-489a-ae59-fd3eaa5bad35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
