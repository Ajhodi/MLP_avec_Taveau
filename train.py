import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Step 1: Load the preprocessed sliding window dataset
DATASET_PATH = "cb513_sliding_window_dataset.csv"  # Path to the dataset generated by Exctract_features_V2.py
dataset = pd.read_csv(DATASET_PATH)

# Step 2: Prepare features (X) and labels (y)
# Convert flattened one-hot encoded features back to numpy arrays
X = np.array(dataset['Features'].apply(lambda x: np.fromstring(x[1:-1], sep=' ')).tolist())
y = dataset['DSSP'].values  # Labels already mapped to numeric format

# Step 3: Split data into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convert labels to categorical format for softmax classification
y_train_cat = to_categorical(y_train, num_classes=3)
y_val_cat = to_categorical(y_val, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

print("Data split into training, validation, and test sets.")

# Step 4: Define the model architecture
def create_model(input_dim):
    model = Sequential([
        Input(shape=(input_dim,)),  # Input layer with the number of features
        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # Add L2 regularization
        Dropout(0.6),  # Dropout for regularization
        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
        Dropout(0.6),
        Dense(3, activation='softmax')  # Output layer for 3-class classification
    ])
    return model

# Step 5: Compile the model
input_dim = X_train.shape[1]
model = create_model(input_dim)
model.compile(optimizer=Adam(learning_rate=0.0005),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print("Model defined and compiled.")

# Step 6: Train the model
batch_size = 16  # Smaller batch size for better generalization
epochs = 30
early_stopping = EarlyStopping(
    monitor='val_loss', patience=3, restore_best_weights=True, verbose=1
)
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss', factor=0.5, patience=2, verbose=1
)

history = model.fit(
    X_train, y_train_cat,
    validation_data=(X_val, y_val_cat),
    batch_size=batch_size,
    epochs=epochs,
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

print("Model training complete.")

# Step 7: Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=1)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Step 8: Save the trained model
model.save("protein_secondary_structure_model.h5")
print("Model saved as 'protein_secondary_structure_model.h5'.")

# Step 9: Visualize Training and Validation Metrics
import matplotlib.pyplot as plt

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
